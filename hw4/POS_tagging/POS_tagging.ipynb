{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqxpid8J3_xt"
   },
   "source": [
    "# NLP Homework 4 Programming Assignment\n",
    "\n",
    "In this assignment, we will train and evaluate a neural model to tag the parts of speech in a sentence.\n",
    "We will also implement several improvements to the model to test its performance.\n",
    "\n",
    "We will be using English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner).\n",
    "\n",
    "## Building a POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X367eCR3_x0"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtnGNDoA3_x3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwA2y6OR3_yE"
   },
   "source": [
    "### Preparing Data\n",
    "We collect the data in the following cell from the `train.txt` and `test.txt` files.  \n",
    "For `train.txt`, we read the word and tag sequences for each sentence. We then create an 80-20 train-val split on this data for training and evaluation purpose.\n",
    "\n",
    "Finally, we are interested in our accuracy on `test.txt`, so we prepare test data from this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "kFpH2P1A3_yG",
    "outputId": "1c889944-290e-4231-9b34-8c07f777aaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  7148\n",
      "Val Data:  1788\n",
      "Test Data:  2012\n",
      "Total tags:  44\n"
     ]
    }
   ],
   "source": [
    "def load_tag_data(tag_file):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    sent = []\n",
    "    tags = []\n",
    "    with open(tag_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                all_sentences.append(sent)\n",
    "                all_tags.append(tags)\n",
    "                sent = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag, _ = line.strip().split()\n",
    "                sent.append(word)\n",
    "                tags.append(tag)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "def load_txt_data(txt_file):\n",
    "    all_sentences = []\n",
    "    sent = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if(line.strip() == \"\"):\n",
    "                all_sentences.append(sent)\n",
    "                sent = []\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                sent.append(word)\n",
    "    return all_sentences\n",
    "\n",
    "train_sentences, train_tags = load_tag_data('train.txt')\n",
    "test_sentences = load_txt_data('test.txt')\n",
    "\n",
    "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
    "\n",
    "# Create train-val split from train data\n",
    "train_val_data = list(zip(train_sentences, train_tags))\n",
    "random.shuffle(train_val_data)\n",
    "split = int(0.8 * len(train_val_data))\n",
    "training_data = train_val_data[:split]\n",
    "val_data = train_val_data[split:]\n",
    "\n",
    "print(\"Train Data: \", len(training_data))\n",
    "print(\"Val Data: \", len(val_data))\n",
    "print(\"Test Data: \", len(test_sentences))\n",
    "print(\"Total tags: \", len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlfliN0J-RzV"
   },
   "source": [
    "### Word-to-Index and Tag-to-Index mapping\n",
    "In order to work with text in Tensor format, we need to map each word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uojEDun83_yP",
    "outputId": "fb218599-7c4b-4b67-cf4d-929e2e8ce2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags 44\n",
      "Vocab size 21589\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "tag_to_idx = {}\n",
    "for tag in unique_tags:\n",
    "    if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "idx_to_tag = {}\n",
    "for tag in tag_to_idx:\n",
    "    idx_to_tag[tag_to_idx[tag]] = tag\n",
    "\n",
    "print(\"Total tags\", len(tag_to_idx))\n",
    "print(\"Vocab size\", len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H26dqorp3_yX"
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(sent, idx_mapping):\n",
    "    idxs = [idx_mapping[word] for word in sent]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRnBTCwD3_yc"
   },
   "source": [
    "### Set up model\n",
    "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence.\n",
    "\n",
    "\n",
    "First we need to define some default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P5SHabu3_yf"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 15\n",
    "HIDDEN_DIM = 6\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkkS4oEb3_yk"
   },
   "source": [
    "### Define Model\n",
    "\n",
    "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. \n",
    "\n",
    "These word embeddings act as input to the LSTM which produces a hidden state. This hidden state is then passed to a Linear layer that produces the probability distribution for the tags of every word. The model will output the tag with the highest probability for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCa30HQb3_ym"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3ab2b0f4ed3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBasicPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBasicPOSTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BasicPOSTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BasicPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size=hidden_dim, num_layers=LSTM_LAYERS, dropout=DROPOUT)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # compute the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ot9J3MrB3_ys"
   },
   "source": [
    "### Training\n",
    "\n",
    "We define train and evaluate procedures that allow us to train our model using our created train-val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWMGxh4Z3_yv"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        \n",
    "        # zero the gradient\n",
    "        model.zero_grad()\n",
    "        # Prepare sentence into indexs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        # Prepare tag into indexs\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        # predictions for the tags of sentence\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        train_examples += len(targets.cpu().detach().numpy())\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function, optimizer):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            # Prepare sentence into indexs\n",
    "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "            # Prepare tag into indexs\n",
    "            targets = prepare_sequence(tags, tag_to_idx)\n",
    "            # predictions for the tags of sentence\n",
    "            tag_scores = model(sentence_in)\n",
    "            # get the prediction results\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "            correct += (torch.sum(preds == torch.LongTensor(targets)).cpu().detach().numpy())\n",
    "            val_examples += len(targets.cpu().detach().numpy())\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsuHjjH1rQeS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\tAvg Train Loss: 0.0787\tAvg Val Loss: 0.0627\t Val Accuracy: 58\n",
      "Time used for Epoch1:  29.361035585403442\n",
      "Epoch: 2/30\tAvg Train Loss: 0.0560\tAvg Val Loss: 0.0534\t Val Accuracy: 64\n",
      "Time used for Epoch2:  27.0801739692688\n",
      "Epoch: 3/30\tAvg Train Loss: 0.0491\tAvg Val Loss: 0.0486\t Val Accuracy: 67\n",
      "Time used for Epoch3:  27.373235940933228\n",
      "Epoch: 4/30\tAvg Train Loss: 0.0449\tAvg Val Loss: 0.0452\t Val Accuracy: 70\n",
      "Time used for Epoch4:  37.17160701751709\n",
      "Epoch: 5/30\tAvg Train Loss: 0.0416\tAvg Val Loss: 0.0424\t Val Accuracy: 72\n",
      "Time used for Epoch5:  69.100022315979\n",
      "Epoch: 6/30\tAvg Train Loss: 0.0389\tAvg Val Loss: 0.0401\t Val Accuracy: 74\n",
      "Time used for Epoch6:  28.22225332260132\n",
      "Epoch: 7/30\tAvg Train Loss: 0.0365\tAvg Val Loss: 0.0382\t Val Accuracy: 75\n",
      "Time used for Epoch7:  25.881731271743774\n",
      "Epoch: 8/30\tAvg Train Loss: 0.0344\tAvg Val Loss: 0.0365\t Val Accuracy: 77\n",
      "Time used for Epoch8:  35.12621259689331\n",
      "Epoch: 9/30\tAvg Train Loss: 0.0326\tAvg Val Loss: 0.0350\t Val Accuracy: 78\n",
      "Time used for Epoch9:  26.88210153579712\n",
      "Epoch: 10/30\tAvg Train Loss: 0.0310\tAvg Val Loss: 0.0337\t Val Accuracy: 79\n",
      "Time used for Epoch10:  79.13855457305908\n",
      "Epoch: 11/30\tAvg Train Loss: 0.0295\tAvg Val Loss: 0.0325\t Val Accuracy: 80\n",
      "Time used for Epoch11:  64.36059379577637\n",
      "Epoch: 12/30\tAvg Train Loss: 0.0281\tAvg Val Loss: 0.0314\t Val Accuracy: 80\n",
      "Time used for Epoch12:  26.25111722946167\n",
      "Epoch: 13/30\tAvg Train Loss: 0.0269\tAvg Val Loss: 0.0304\t Val Accuracy: 81\n",
      "Time used for Epoch13:  25.756081342697144\n",
      "Epoch: 14/30\tAvg Train Loss: 0.0257\tAvg Val Loss: 0.0296\t Val Accuracy: 82\n",
      "Time used for Epoch14:  49.210633516311646\n",
      "Epoch: 15/30\tAvg Train Loss: 0.0246\tAvg Val Loss: 0.0289\t Val Accuracy: 82\n",
      "Time used for Epoch15:  26.510480165481567\n",
      "Epoch: 16/30\tAvg Train Loss: 0.0236\tAvg Val Loss: 0.0282\t Val Accuracy: 83\n",
      "Time used for Epoch16:  30.905997276306152\n",
      "Epoch: 17/30\tAvg Train Loss: 0.0227\tAvg Val Loss: 0.0277\t Val Accuracy: 83\n",
      "Time used for Epoch17:  39.78366947174072\n",
      "Epoch: 18/30\tAvg Train Loss: 0.0218\tAvg Val Loss: 0.0272\t Val Accuracy: 84\n",
      "Time used for Epoch18:  26.35568881034851\n",
      "Epoch: 19/30\tAvg Train Loss: 0.0210\tAvg Val Loss: 0.0268\t Val Accuracy: 84\n",
      "Time used for Epoch19:  29.78670358657837\n",
      "Epoch: 20/30\tAvg Train Loss: 0.0203\tAvg Val Loss: 0.0263\t Val Accuracy: 85\n",
      "Time used for Epoch20:  33.0583119392395\n",
      "Epoch: 21/30\tAvg Train Loss: 0.0196\tAvg Val Loss: 0.0260\t Val Accuracy: 85\n",
      "Time used for Epoch21:  26.222330808639526\n",
      "Epoch: 22/30\tAvg Train Loss: 0.0190\tAvg Val Loss: 0.0256\t Val Accuracy: 85\n",
      "Time used for Epoch22:  46.80946397781372\n",
      "Epoch: 23/30\tAvg Train Loss: 0.0183\tAvg Val Loss: 0.0253\t Val Accuracy: 86\n",
      "Time used for Epoch23:  30.474191904067993\n",
      "Epoch: 24/30\tAvg Train Loss: 0.0178\tAvg Val Loss: 0.0250\t Val Accuracy: 86\n",
      "Time used for Epoch24:  27.368995666503906\n",
      "Epoch: 25/30\tAvg Train Loss: 0.0172\tAvg Val Loss: 0.0247\t Val Accuracy: 86\n",
      "Time used for Epoch25:  45.78583645820618\n",
      "Epoch: 26/30\tAvg Train Loss: 0.0166\tAvg Val Loss: 0.0245\t Val Accuracy: 86\n",
      "Time used for Epoch26:  45.65144944190979\n",
      "Epoch: 27/30\tAvg Train Loss: 0.0161\tAvg Val Loss: 0.0243\t Val Accuracy: 87\n",
      "Time used for Epoch27:  51.79115152359009\n",
      "Epoch: 28/30\tAvg Train Loss: 0.0157\tAvg Val Loss: 0.0241\t Val Accuracy: 87\n",
      "Time used for Epoch28:  31.958281993865967\n",
      "Epoch: 29/30\tAvg Train Loss: 0.0152\tAvg Val Loss: 0.0240\t Val Accuracy: 87\n",
      "Time used for Epoch29:  32.88745832443237\n",
      "Epoch: 30/30\tAvg Train Loss: 0.0148\tAvg Val Loss: 0.0238\t Val Accuracy: 87\n",
      "Time used for Epoch30:  27.47619318962097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu_user/.conda/envs/pytorch/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type BasicPOSTagger. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = BasicPOSTagger(embedding_dim=EMBEDDING_DIM, \n",
    "                       hidden_dim=HIDDEN_DIM, \n",
    "                       vocab_size = len(word_to_idx), \n",
    "                       tagset_size = len(tag_to_idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    start = time.time()\n",
    "    train(epoch, model, loss_function, optimizer)\n",
    "    print(f\"Time used for Epoch{epoch}: \",time.time() - start)\n",
    "torch.save(model, \"basic_pos_tagger.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uK6mT_k8NRvB"
   },
   "source": [
    "You should get a performance of **at least 80%** on the validation set for the BasicPOSTagger.\n",
    "\n",
    "Let us now write a method to save our predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2e9aoWNcNomd"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "            tag_scores = model(sentence_in)\n",
    "            preds = tag_scores.argmax(axis=1)\n",
    "            predicted_tags.extend([idx_to_tag[preds[i].item()] for i in range(len(preds))])\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azW08GfZSHcQ"
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9T6s3XTFG46"
   },
   "source": [
    "\n",
    "### Test accuracy\n",
    "Evaluate your performance on the test data by submitting test_labels.txt generated by the method above and **report your test accuracy here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iP64WDReBuDr"
   },
   "source": [
    "Imitate the above method to generate prediction for validation data.\n",
    "Create lists of words, tags predicted by the model and ground truth tags. \n",
    "\n",
    "Use these lists to carry out error analysis to find the top-10 types of errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QgMHr7HCn1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NN', 'NNP', 0.056300774607972795, ['Whitten', 'Arthur', 'Education', 'Herman', 'Hoyt'])\n",
      "('JJ', 'NN', 0.0477989797846212, ['verge', 'worth', 'span', 'lie', 'chief'])\n",
      "('NN', 'JJ', 0.04402040430757605, ['literary', 'same', 'fastest-growing', 'Western', 'peaceful'])\n",
      "('NNP', 'NN', 0.03117324768562252, ['overhead', 'province', 'provision', 'democratization', 'rent'])\n",
      "('JJ', 'NNP', 0.029850746268656716, ['American', 'Lyneses', 'Wolf', 'Neuhaus', 'Chris-Craft'])\n",
      "('NNS', 'NNP', 0.02550538447005479, ['Thanh', 'Galicia', 'Cordis', 'Amex', 'PATOIS'])\n",
      "('NNP', 'NNS', 0.024371811826941245, ['sources', 'onlookers', 'barricades', 'buildings', 'supports'])\n",
      "('NNP', 'JJ', 0.023238239183827697, ['nonprofit', 'less-developed', 'nearby', 'perturbed', 'equitable'])\n",
      "('NN', 'NNS', 0.02153788021915738, ['donors', 'rows', 'foundations', 'buffs', 'aspirations'])\n",
      "('VBN', 'VBD', 0.019837521254487057, ['stepped', 'told', 'managed', 'ordered', 'used'])\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, test_sentences):\n",
    "    word_list = []\n",
    "    model_tags = []\n",
    "    gt_tags = []\n",
    "    for sentence, tags in test_sentences:\n",
    "        sentence_trans = prepare_sequence(sentence, word_to_idx)\n",
    "        tag_scores = model(sentence_trans)  \n",
    "        preds = tag_scores.argmax(axis=1)\n",
    "        model_tags.extend([idx_to_tag[i.item()] for i in preds])\n",
    "\n",
    "        word_list.extend(sentence)\n",
    "        gt_tags.extend(tags)\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    from collections import Counter, defaultdict\n",
    "    \n",
    "    wl = []\n",
    "    mt = []\n",
    "    gt = []\n",
    "    words = defaultdict(list)\n",
    "    \n",
    "    for w, m, g in zip(word_list, model_tags, gt_tags):\n",
    "        if m != g:\n",
    "            wl.append(w)\n",
    "            mt.append(m)\n",
    "            gt.append(g)\n",
    "            words[(m, g)].append(w)\n",
    "            \n",
    "    c = Counter(zip(mt, gt))\n",
    "    top10 = c.most_common(10)\n",
    "    \n",
    "    errors = []\n",
    "    for (mtag, gt_tag), count in top10:\n",
    "        errors.append((mtag, gt_tag, count / sum(c.values()), words[(mtag, gt_tag)][:5]))\n",
    "    return errors\n",
    "\n",
    "errors = error_analysis(*generate_predictions(model, val_data))\n",
    "\n",
    "for err in errors:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRNjFRDcD2h7"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "What kinds of errors did the model make and why do you think it made them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the results, we could see with the basic pos tagger using words, the NN, NNP and JJ are confusing model since their position in the sentence often similar. Also, the model also make mistakes on discern different type of noun. \n",
    "The reason is that we use one direction LSTM which do not have enough clue to separate them by the similar tagging structure. The accuracy achieved 86.23%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svXyUssdXZ4r"
   },
   "source": [
    "## Define a Character Level POS Tagger\n",
    "\n",
    "We can use the character-level information present to augment our word embeddings. Words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, we can run a character level LSTM on every word (treated as a tensor of characters, each mapped to character-index space) to create a character-level representation of the word. This representation can be concatenated with the word embedding (as in the BasicPOSTagger) to create a new word embedding that captures more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX4-3AoxSJeY"
   },
   "outputs": [],
   "source": [
    "# Create char to index mapping\n",
    "char_to_idx = {}\n",
    "unique_chars = set()\n",
    "MAX_WORD_LEN = 0\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            unique_chars.add(c)\n",
    "        if len(word) > MAX_WORD_LEN:\n",
    "            MAX_WORD_LEN = len(word)\n",
    "\n",
    "for c in unique_chars:\n",
    "    char_to_idx[c] = len(char_to_idx)\n",
    "char_to_idx[' '] = len(char_to_idx)\n",
    "\n",
    "# New Hyperparameters\n",
    "# EMBEDDING_DIM = 16\n",
    "# HIDDEN_DIM = 8\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 16\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 12\n",
    "CHAR_EMBEDDING_DIM = 4\n",
    "CHAR_HIDDEN_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7U0wb4OeOsde"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class CharPOSTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "        super(CharPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an char level LSTM: that finds the character level embedding for a word\n",
    "        # an LSTM layer: that takes the combined embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        # word embedding\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_word = nn.LSTM(embedding_dim, self.hidden_dim)\n",
    "        \n",
    "        # char embedding\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "        self.char_embedding = nn.Embedding(char_size, char_embedding_dim)\n",
    "        self.lstm_char = nn.LSTM(char_embedding_dim, self.char_hidden_dim)\n",
    "        \n",
    "        # combine the word / character\n",
    "        self.overall_hidden_dim = hidden_dim + MAX_WORD_LEN * char_hidden_dim\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(self.overall_hidden_dim, tagset_size)\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################    \n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embeds = self.word_embedding(sentence)\n",
    "        lstm_out, _ = self.lstm_word(embeds.view(len(sentence), 1, -1))\n",
    "        \n",
    "        embeds_char = self.char_embedding(chars)\n",
    "        char_lstm_out, _ = self.lstm_char(embeds_char.view(len(chars), 1, -1))\n",
    "        \n",
    "        # Remember!!!!!!! You Should re-organized the characters into sentence!!!!!!!!!!\n",
    "        merge_out = torch.cat((lstm_out.view(len(sentence), -1), char_lstm_out.view(len(sentence), -1)), 1)\n",
    "        \n",
    "        tag_space = self.hidden2tag(merge_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        tag_space = self.hidden2tag(merge_out)\n",
    "        tag_scores = F.log_softmax(tag_scores, dim=1)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores\n",
    "\n",
    "def train_char(epoch, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    s_idx = 0\n",
    "    for sentence, tags in tqdm(training_data):\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences as well as character sequences. Find the gradient with \n",
    "        # respect to the loss and update the model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get input for the model\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        sentence_chars = []\n",
    "        for w in sentence:\n",
    "            spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sentence_chars.extend(list(spaces + w))\n",
    "        char_in = prepare_sequence(sentence_chars, char_to_idx)\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        \n",
    "        tag_scores = model(sentence_in, char_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        train_examples += len(targets.cpu().detach().numpy())\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function, optimizer)\n",
    "\n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate_char(model, loss_function, optimizer):\n",
    "    # returns:: avg_val_loss (float)\n",
    "    # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "\n",
    "            # Get input for the model\n",
    "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "            sentence_chars = []\n",
    "            for w in sentence:\n",
    "                spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "                sentence_chars.extend(list(spaces + w))\n",
    "            char_in = prepare_sequence(sentence_chars, char_to_idx)\n",
    "            targets = prepare_sequence(tags, tag_to_idx)\n",
    "\n",
    "            tag_scores = model(sentence_in, char_in)\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            \n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "            correct += (torch.sum(preds == torch.LongTensor(targets)).cpu().detach().numpy())\n",
    "            val_examples += len(targets.cpu().detach().numpy())\n",
    "            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-QttCw6Otf-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/7148 [00:02<21:51,  5.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1e0763608ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time used for Epoch{epoch}: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-88a0c96be839>\u001b[0m in \u001b[0;36mtrain_char\u001b[0;34m(epoch, model, loss_function, optimizer)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-88a0c96be839>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence, chars)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0membeds_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mchar_lstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds_char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Remember!!!!!!! You Should re-organized the characters into sentence!!!!!!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = CharPOSTagger(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                       char_hidden_dim = CHAR_HIDDEN_DIM, char_embedding_dim = CHAR_EMBEDDING_DIM,\n",
    "                       char_size = len(char_to_idx), vocab_size = len(word_to_idx), tagset_size = len(tag_to_idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    train_char(epoch, model, loss_function, optimizer)\n",
    "    print(f\"Time used for Epoch{epoch}: \",time.time() - start)\n",
    "    if epoch % 2 == 0:\n",
    "        torch.save(model, f\"char_pos_tagger_v2_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"char_pos_tagger_8.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            try:\n",
    "                sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "                sentence_chars = []\n",
    "                for w in sentence:\n",
    "                    spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "                    sentence_chars.extend(list(spaces + w))\n",
    "                char_in = prepare_sequence(sentence_chars, char_to_idx)\n",
    "                tag_scores = model(sentence_in, char_in)\n",
    "                preds = tag_scores.argmax(axis=1)\n",
    "                predicted_tags.extend([idx_to_tag[preds[i].item()] for i in range(len(preds))])\n",
    "            except:\n",
    "                pass\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels_char.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xslNYW8EBKMQ"
   },
   "source": [
    "Tune your hyperparameters, to get a performance of **at least 85%** on the validation set for the CharPOSTagger.\n",
    "\n",
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuLl_BSMeovb"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du0raTJreqT2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('POS', 'DT', 0.08363784979324936, ['the', 'the', 'the', 'the', 'no'])\n",
      "('VBG', 'IN', 0.07979132608904703, ['in', 'In', 'off', 'at', 'in'])\n",
      "('WDT', 'NN', 0.06440523127223771, ['overhead', 'province', 'rice', 'program', 'program'])\n",
      "('WDT', 'NNP', 0.05442831041446293, ['Peterson', 'Service', 'Whitten', 'James', 'B.A.T'])\n",
      "('WRB', ',', 0.05113472449273969, [',', ',', ',', ',', ','])\n",
      "('(', 'NN', 0.04459563419559573, ['suit', 'fact', 'money', 'panhandler', 'night'])\n",
      "('CD', '.', 0.04228771997307433, ['.', '.', '.', '.', '.'])\n",
      "('WDT', 'NNS', 0.04019617270891432, ['companies', 'officials', 'villagers', 'sponsors', 'regulators'])\n",
      "('WDT', 'JJ', 0.03185402442542552, ['tax-collection', 'fiscal', 'less-developed', 'third-quarter', 'institutional'])\n",
      "('RBS', 'TO', 0.0233916722761804, ['to', 'to', 'to', 'to', 'to'])\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, test_sentences):\n",
    "    word_list = []\n",
    "    model_tags = []\n",
    "    gt_tags = []\n",
    "    for sentence, tags in test_sentences:\n",
    "        \n",
    "        sentence_chars = []\n",
    "        for w in sentence:\n",
    "            spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sentence_chars.extend(list(spaces + w))\n",
    "        char_in = prepare_sequence(sentence_chars, char_to_idx)\n",
    "        sentence_trans = prepare_sequence(sentence, word_to_idx)\n",
    "        tag_scores = model(sentence_trans, char_in)  \n",
    "        preds = tag_scores.argmax(axis=1)\n",
    "        model_tags.extend([idx_to_tag[i.item()] for i in preds])\n",
    "\n",
    "        word_list.extend(sentence)\n",
    "        gt_tags.extend(tags)\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    from collections import Counter, defaultdict\n",
    "    \n",
    "    wl = []\n",
    "    mt = []\n",
    "    gt = []\n",
    "    words = defaultdict(list)\n",
    "    \n",
    "    for w, m, g in zip(word_list, model_tags, gt_tags):\n",
    "        if m != g:\n",
    "            wl.append(w)\n",
    "            mt.append(m)\n",
    "            gt.append(g)\n",
    "            words[(m, g)].append(w)\n",
    "            \n",
    "    c = Counter(zip(mt, gt))\n",
    "    top10 = c.most_common(10)\n",
    "    \n",
    "    errors = []\n",
    "    for (mtag, gt_tag), count in top10:\n",
    "        errors.append((mtag, gt_tag, count / sum(c.values()), words[(mtag, gt_tag)][:5]))\n",
    "    return errors\n",
    "\n",
    "errors = error_analysis(*generate_predictions(model, val_data))\n",
    "\n",
    "for err in errors:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GEP-IgiESzN"
   },
   "source": [
    "\n",
    "**Report your findings here.**  \n",
    "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the LSTM with character information, we could see that confusion of different noun has been improved. \n",
    "The final accuracy is 90.74%.\n",
    "However, punctuation marks become some of the error. \n",
    "And the error occurred when there are similar word structure like or similar end of the word. \n",
    "The reason might be the information provided by characters is over-weighted than the sentence structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQdc3gH8d_a4"
   },
   "source": [
    "## Define a BiLSTM POS Tagger\n",
    "\n",
    "A bidirectional LSTM that runs both left-to-right and right-to-left to represent dependencies between adjacent words in both directions and thus captures dependencies in both directions. \n",
    "\n",
    "In this part, you make your model bidirectional. \n",
    "\n",
    "In addition, you should implement one of these modifications to improve the model's performance:\n",
    "- Tune the model hyperparameters. Try at least 5 different combinations of parameters. For example:\n",
    "    - number of LSTM layers\n",
    "    - number of hidden dimensions\n",
    "    - number of word embedding dimensions\n",
    "    - dropout rate\n",
    "    - learning rate\n",
    "- Switch to pre-trained Word Embeddings instead of training them from scratch. Try at least one different embedding method. For example:\n",
    "    - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "    - [Fast Text](https://fasttext.cc/docs/en/english-vectors.html)\n",
    "- Implement a different model architecture. Try at least one different architecture. For example:\n",
    "    - adding a conditional random field on top of the LSTM\n",
    "    - adding Viterbi decoding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext as text\n",
    "\n",
    "# Set 1: 92% acc\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 32\n",
    "LEARNING_RATE = 0.01\n",
    "BIDIRECTIONAL = True\n",
    "LSTM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 12\n",
    "\n",
    "# # Set 2: 79% acc\n",
    "# EMBEDDING_DIM = 200\n",
    "# HIDDEN_DIM = 16\n",
    "# LEARNING_RATE = 0.01\n",
    "# BIDIRECTIONAL = True\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0.5\n",
    "# EPOCHS = 20\n",
    "\n",
    "# Set 3: 86% acc\n",
    "# EMBEDDING_DIM = 200\n",
    "# HIDDEN_DIM = 32\n",
    "# LEARNING_RATE = 0.1\n",
    "# BIDIRECTIONAL = True\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0.3\n",
    "# EPOCHS = 30\n",
    "\n",
    "# Set 4: 83%\n",
    "# EMBEDDING_DIM = 200\n",
    "# HIDDEN_DIM = 32\n",
    "# LEARNING_RATE = 0.001\n",
    "# BIDIRECTIONAL = True\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0\n",
    "# EPOCHS = 30\n",
    "\n",
    "# Set 5:\n",
    "# EMBEDDING_DIM = 200\n",
    "# HIDDEN_DIM = 32\n",
    "# LEARNING_RATE = 0.1\n",
    "# BIDIRECTIONAL = True\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0\n",
    "# EPOCHS = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zo-TWvcYeHhT"
   },
   "outputs": [],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    # NOTE: you may have to modify these function headers to include your \n",
    "    # modification, e.g. adding a parameter for embeddings data\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BiLSTMPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # a BiLSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.vec = text.vocab.GloVe(name='6B', dim=embedding_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size=hidden_dim, \n",
    "                        num_layers=LSTM_LAYERS, \n",
    "                        dropout = DROPOUT if LSTM_LAYERS > 1 else 0,bidirectional=BIDIRECTIONAL)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2 if BIDIRECTIONAL else hidden_dim, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embeds = self.vec.get_vecs_by_tokens(sentence, lower_case_backup=True)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in tqdm(training_data):\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        \n",
    "        # zero the gradient\n",
    "        model.zero_grad()\n",
    "        # Prepare sentence into indexs\n",
    "#         sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        sentence4embed = sentence\n",
    "        # Prepare tag into indexs\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        # predictions for the tags of sentence\n",
    "        tag_scores = model(sentence4embed)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        train_examples += len(targets.cpu().detach().numpy())\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function, optimizer):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            # Prepare sentence into indexs\n",
    "            sentence4embed = sentence\n",
    "            # Prepare tag into indexs\n",
    "            targets = prepare_sequence(tags, tag_to_idx)\n",
    "            # predictions for the tags of sentence\n",
    "            tag_scores = model(sentence4embed)\n",
    "            # get the prediction results\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "            correct += (torch.sum(preds == torch.LongTensor(targets)).cpu().detach().numpy())\n",
    "            val_examples += len(targets.cpu().detach().numpy())\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mc2d_0k6mktK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:34<00:00, 33.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/12\tAvg Train Loss: 0.0203\tAvg Val Loss: 0.0145\t Val Accuracy: 90\n",
      "Time used for Epoch1:  221.84125971794128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu_user/.conda/envs/pytorch/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type BiLSTMPOSTagger. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "100%|██████████| 7148/7148 [03:20<00:00, 35.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/12\tAvg Train Loss: 0.0125\tAvg Val Loss: 0.0132\t Val Accuracy: 91\n",
      "Time used for Epoch2:  205.65346002578735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [02:48<00:00, 42.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/12\tAvg Train Loss: 0.0111\tAvg Val Loss: 0.0119\t Val Accuracy: 92\n",
      "Time used for Epoch3:  175.02353739738464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:32<00:00, 33.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/12\tAvg Train Loss: 0.0101\tAvg Val Loss: 0.0124\t Val Accuracy: 92\n",
      "Time used for Epoch4:  219.23559665679932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:33<00:00, 33.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/12\tAvg Train Loss: 0.0098\tAvg Val Loss: 0.0117\t Val Accuracy: 92\n",
      "Time used for Epoch5:  220.40116548538208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:52<00:00, 30.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/12\tAvg Train Loss: 0.0094\tAvg Val Loss: 0.0113\t Val Accuracy: 92\n",
      "Time used for Epoch6:  239.92650365829468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:32<00:00, 33.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/12\tAvg Train Loss: 0.0090\tAvg Val Loss: 0.0119\t Val Accuracy: 92\n",
      "Time used for Epoch7:  219.8170702457428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [04:02<00:00, 29.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/12\tAvg Train Loss: 0.0090\tAvg Val Loss: 0.0113\t Val Accuracy: 93\n",
      "Time used for Epoch8:  248.811377286911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:23<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/12\tAvg Train Loss: 0.0087\tAvg Val Loss: 0.0109\t Val Accuracy: 93\n",
      "Time used for Epoch9:  210.94725704193115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [04:04<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/12\tAvg Train Loss: 0.0087\tAvg Val Loss: 0.0113\t Val Accuracy: 92\n",
      "Time used for Epoch10:  251.02008819580078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:41<00:00, 32.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/12\tAvg Train Loss: 0.0086\tAvg Val Loss: 0.0107\t Val Accuracy: 93\n",
      "Time used for Epoch11:  228.0372278690338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7148/7148 [03:38<00:00, 32.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/12\tAvg Train Loss: 0.0081\tAvg Val Loss: 0.0105\t Val Accuracy: 93\n",
      "Time used for Epoch12:  224.1796362400055\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = BiLSTMPOSTagger(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                       vocab_size = len(word_to_idx), tagset_size = len(tag_to_idx))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    start = time.time()\n",
    "    train(epoch, model, loss_function, optimizer)\n",
    "    print(f\"Time used for Epoch{epoch}: \",time.time() - start)\n",
    "    torch.save(model, \"bi_pos_tagger-exp1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6NwhSXaBCNl"
   },
   "source": [
    "Your modified model should get a performance of **at least 90%** on the validation set.\n",
    "\n",
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            sentence_in = sentence\n",
    "            tag_scores = model(sentence_in)\n",
    "            preds = tag_scores.argmax(axis=1)\n",
    "            predicted_tags.extend([idx_to_tag[preds[i].item()] for i in range(len(preds))])\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels_bilstm.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl2C26leFihB"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "Compare the top-10 errors made by this modified model with the errors made by the model from part (a). \n",
    "If you tried multiple hyperparameter combinations, choose the model with the highest validation data accuracy.\n",
    "What errors does the original model make as compared to the modified model, and why do you think it made them? \n",
    "\n",
    "Feel free to reuse the methods defined above for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kh0S5yXIA_0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NN', 'NNP', 0.1099092812281926, ['Service', 'Education', 'Bush', 'Disneyland', 'Paper'])\n",
      "('JJ', 'NN', 0.061409630146545706, ['panhandler', 'stuff', 'estuarian', 'gamma', 'eclectic'])\n",
      "('NN', 'JJ', 0.05722260990928123, ['nonprofit', 'necessary', 'literary', 'environmental', 'savings-and-loan'])\n",
      "('NNP', 'NN', 0.05198883461270063, ['province', 'bureau', 'press', 'secretary', 'rent'])\n",
      "('JJ', 'NNP', 0.047801814375436145, ['Thanh', 'Hoa', 'Extension', 'Third', 'East'])\n",
      "('NNS', 'NN', 0.027564549895324496, ['list', 'worth', 'globulin', 'fluoride', 'hedge'])\n",
      "('VBD', 'VBN', 0.026866713189113746, ['confiscated', 'ended', 'led', 'caused', 'ended'])\n",
      "('NN', 'VBG', 0.026168876482903, ['setting', 'operating', 'operating', 'driving', 'sitting'])\n",
      "('NN', 'NNS', 0.02407536636427076, ['write-downs', 'market-makers', 'doldrums', 'placements', 'woods'])\n",
      "('NNP', 'JJ', 0.019190509420795535, ['universal', 'electric', '30-share', 'first', 'mainframe-class'])\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, test_sentences):\n",
    "    word_list = []\n",
    "    model_tags = []\n",
    "    gt_tags = []\n",
    "    for sentence, tags in test_sentences:\n",
    "        sentence_idx = prepare_sequence(sentence, word_to_idx)\n",
    "        sentence_trans = sentence\n",
    "        tag_scores = model(sentence_trans)  \n",
    "        preds = tag_scores.argmax(axis=1)\n",
    "        model_tags.extend([idx_to_tag[i.item()] for i in preds])\n",
    "\n",
    "        word_list.extend(sentence)\n",
    "        gt_tags.extend(tags)\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    from collections import Counter, defaultdict\n",
    "    \n",
    "    wl = []\n",
    "    mt = []\n",
    "    gt = []\n",
    "    words = defaultdict(list)\n",
    "    \n",
    "    for w, m, g in zip(word_list, model_tags, gt_tags):\n",
    "        if m != g:\n",
    "            wl.append(w)\n",
    "            mt.append(m)\n",
    "            gt.append(g)\n",
    "            words[(m, g)].append(w)\n",
    "            \n",
    "    c = Counter(zip(mt, gt))\n",
    "    top10 = c.most_common(10)\n",
    "    \n",
    "    errors = []\n",
    "    for (mtag, gt_tag), count in top10:\n",
    "        errors.append((mtag, gt_tag, count / sum(c.values()), words[(mtag, gt_tag)][:5]))\n",
    "    return errors\n",
    "\n",
    "errors = error_analysis(*generate_predictions(model, val_data))\n",
    "\n",
    "for err in errors:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I try to use different pertained embedded weight called Glove with dimension of 200. With trying different hyper-parameters, I found that with proper dropout rate like 0.1 and learning rate 0.01 is the good way not to overfitting. The final accuracy on the validation set achieved 93 %."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POS_tagging.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}