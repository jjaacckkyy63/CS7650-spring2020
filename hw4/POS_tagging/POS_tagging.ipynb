{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqxpid8J3_xt"
   },
   "source": [
    "# NLP Homework 4 Programming Assignment\n",
    "\n",
    "In this assignment, we will train and evaluate a neural model to tag the parts of speech in a sentence.\n",
    "We will also implement several improvements to the model to test its performance.\n",
    "\n",
    "We will be using English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner).\n",
    "\n",
    "## Building a POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X367eCR3_x0"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtnGNDoA3_x3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwA2y6OR3_yE"
   },
   "source": [
    "### Preparing Data\n",
    "We collect the data in the following cell from the `train.txt` and `test.txt` files.  \n",
    "For `train.txt`, we read the word and tag sequences for each sentence. We then create an 80-20 train-val split on this data for training and evaluation purpose.\n",
    "\n",
    "Finally, we are interested in our accuracy on `test.txt`, so we prepare test data from this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "kFpH2P1A3_yG",
    "outputId": "1c889944-290e-4231-9b34-8c07f777aaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  7148\n",
      "Val Data:  1788\n",
      "Test Data:  2012\n",
      "Total tags:  44\n"
     ]
    }
   ],
   "source": [
    "def load_tag_data(tag_file):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    sent = []\n",
    "    tags = []\n",
    "    with open(tag_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                all_sentences.append(sent)\n",
    "                all_tags.append(tags)\n",
    "                sent = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag, _ = line.strip().split()\n",
    "                sent.append(word)\n",
    "                tags.append(tag)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "def load_txt_data(txt_file):\n",
    "    all_sentences = []\n",
    "    sent = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if(line.strip() == \"\"):\n",
    "                all_sentences.append(sent)\n",
    "                sent = []\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                sent.append(word)\n",
    "    return all_sentences\n",
    "\n",
    "train_sentences, train_tags = load_tag_data('train.txt')\n",
    "test_sentences = load_txt_data('test.txt')\n",
    "\n",
    "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
    "\n",
    "# Create train-val split from train data\n",
    "train_val_data = list(zip(train_sentences, train_tags))\n",
    "random.shuffle(train_val_data)\n",
    "split = int(0.8 * len(train_val_data))\n",
    "training_data = train_val_data[:split]\n",
    "val_data = train_val_data[split:]\n",
    "\n",
    "print(\"Train Data: \", len(training_data))\n",
    "print(\"Val Data: \", len(val_data))\n",
    "print(\"Test Data: \", len(test_sentences))\n",
    "print(\"Total tags: \", len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlfliN0J-RzV"
   },
   "source": [
    "### Word-to-Index and Tag-to-Index mapping\n",
    "In order to work with text in Tensor format, we need to map each word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uojEDun83_yP",
    "outputId": "fb218599-7c4b-4b67-cf4d-929e2e8ce2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags 44\n",
      "Vocab size 21589\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "tag_to_idx = {}\n",
    "for tag in unique_tags:\n",
    "    if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "idx_to_tag = {}\n",
    "for tag in tag_to_idx:\n",
    "    idx_to_tag[tag_to_idx[tag]] = tag\n",
    "\n",
    "print(\"Total tags\", len(tag_to_idx))\n",
    "print(\"Vocab size\", len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H26dqorp3_yX"
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(sent, idx_mapping):\n",
    "    idxs = [idx_mapping[word] for word in sent]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRnBTCwD3_yc"
   },
   "source": [
    "### Set up model\n",
    "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence.\n",
    "\n",
    "\n",
    "First we need to define some default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P5SHabu3_yf"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM = 10\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkkS4oEb3_yk"
   },
   "source": [
    "### Define Model\n",
    "\n",
    "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. \n",
    "\n",
    "These word embeddings act as input to the LSTM which produces a hidden state. This hidden state is then passed to a Linear layer that produces the probability distribution for the tags of every word. The model will output the tag with the highest probability for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCa30HQb3_ym"
   },
   "outputs": [],
   "source": [
    "class BasicPOSTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BasicPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size=hidden_dim, num_layers=LSTM_LAYERS, dropout=DROPOUT)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # compute the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ot9J3MrB3_ys"
   },
   "source": [
    "### Training\n",
    "\n",
    "We define train and evaluate procedures that allow us to train our model using our created train-val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWMGxh4Z3_yv"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        \n",
    "        # zero the gradient\n",
    "        model.zero_grad()\n",
    "        # Prepare sentence into indexs\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        # Prepare tag into indexs\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        # predictions for the tags of sentence\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        train_examples += len(targets.cpu().detach().numpy())\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function, optimizer):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            # Prepare sentence into indexs\n",
    "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "            # Prepare tag into indexs\n",
    "            targets = prepare_sequence(tags, tag_to_idx)\n",
    "            # predictions for the tags of sentence\n",
    "            tag_scores = model(sentence_in)\n",
    "            # get the prediction results\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "            correct += (torch.sum(preds == torch.LongTensor(targets)).cpu().detach().numpy())\n",
    "            val_examples += len(targets.cpu().detach().numpy())\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsuHjjH1rQeS"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BasicPOSTagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64076d0b83a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: Initialize the model, optimizer and the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = BasicPOSTagger(embedding_dim=EMBEDDING_DIM, \n\u001b[0m\u001b[1;32m      5\u001b[0m                        \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                        \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BasicPOSTagger' is not defined"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = BasicPOSTagger(embedding_dim=EMBEDDING_DIM, \n",
    "                       hidden_dim=HIDDEN_DIM, \n",
    "                       vocab_size = len(word_to_idx), \n",
    "                       tagset_size = len(tag_to_idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    start = time.time()\n",
    "    train(epoch, model, loss_function, optimizer)\n",
    "    print(f\"Time used for Epoch{epoch}: \",time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uK6mT_k8NRvB"
   },
   "source": [
    "You should get a performance of **at least 80%** on the validation set for the BasicPOSTagger.\n",
    "\n",
    "Let us now write a method to save our predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2e9aoWNcNomd"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "            tag_scores = model(sentence_in)\n",
    "            \n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "\n",
    "            for pred in preds.tolist():\n",
    "                predicted_tags.append(idx_to_tag[pred])\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azW08GfZSHcQ"
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9T6s3XTFG46"
   },
   "source": [
    "\n",
    "### Test accuracy\n",
    "Evaluate your performance on the test data by submitting test_labels.txt generated by the method above and **report your test accuracy here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iP64WDReBuDr"
   },
   "source": [
    "Imitate the above method to generate prediction for validation data.\n",
    "Create lists of words, tags predicted by the model and ground truth tags. \n",
    "\n",
    "Use these lists to carry out error analysis to find the top-10 types of errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QgMHr7HCn1x"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, test_sentences):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRNjFRDcD2h7"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "What kinds of errors did the model make and why do you think it made them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svXyUssdXZ4r"
   },
   "source": [
    "## Define a Character Level POS Tagger\n",
    "\n",
    "We can use the character-level information present to augment our word embeddings. Words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, we can run a character level LSTM on every word (treated as a tensor of characters, each mapped to character-index space) to create a character-level representation of the word. This representation can be concatenated with the word embedding (as in the BasicPOSTagger) to create a new word embedding that captures more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX4-3AoxSJeY"
   },
   "outputs": [],
   "source": [
    "# Create char to index mapping\n",
    "char_to_idx = {}\n",
    "unique_chars = set()\n",
    "MAX_WORD_LEN = 0\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            unique_chars.add(c)\n",
    "        if len(word) > MAX_WORD_LEN:\n",
    "            MAX_WORD_LEN = len(word)\n",
    "\n",
    "for c in unique_chars:\n",
    "    char_to_idx[c] = len(char_to_idx)\n",
    "char_to_idx[' '] = len(char_to_idx)\n",
    "\n",
    "# New Hyperparameters\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 3\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 30\n",
    "CHAR_EMBEDDING_DIM = 3\n",
    "CHAR_HIDDEN_DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7U0wb4OeOsde"
   },
   "outputs": [],
   "source": [
    "class CharPOSTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "        super(CharPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an char level LSTM: that finds the character level embedding for a word\n",
    "        # an LSTM layer: that takes the combined embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        # word embedding\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_word = nn.LSTM(embedding_dim, self.hidden_dim)\n",
    "        \n",
    "        # char embedding\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "        self.char_embedding = nn.Embedding(char_size, char_embedding_dim)\n",
    "        self.lstm_char = nn.LSTM(char_embedding_dim, self.char_hidden_dim)\n",
    "        \n",
    "        # combine the word / character\n",
    "        self.overall_hidden_dim = hidden_dim + MAX_WORD_LEN * char_hidden_dim\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(self.overall_hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.char_hidden = self.init_hidden(isChar=True)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def init_hidden(self, isChar=False):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        if isChar:\n",
    "            return (torch.autograd.Variable(torch.zeros(1, 1, self.char_hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, 1, self.char_hidden_dim)))\n",
    "        else:\n",
    "            return (torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                torch.autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))        \n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embeds = self.word_embedding(sentence)\n",
    "        lstm_out, self.hidden = self.lstm_word(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        embeds_char = self.char_embedding(chars)\n",
    "        char_lstm_out, self.char_hidden = self.lstm_char(embeds_char.view(len(chars), 1, -1), self.char_hidden)\n",
    "        \n",
    "        # Remember!!!!!!! You Should re-organized the characters into sentence!!!!!!!!!!\n",
    "        merge_out = torch.cat((lstm_out.view(len(sentence), -1), char_lstm_out.view(len(sentence), -1)), 1)\n",
    "        \n",
    "        tag_space = self.hidden2tag(merge_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        tag_space = self.hidden2tag(merge_out)\n",
    "        tag_scores = F.log_softmax(tag_scores, dim=1)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores\n",
    "\n",
    "def train_char(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences as well as character sequences. Find the gradient with \n",
    "        # respect to the loss and update the model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # initiate the hidden state\n",
    "        model.hidden = model.init_hidden()\n",
    "        model.char_hidden = model.init_hidden(isChar=True)\n",
    "        \n",
    "        # Get input for the model\n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        sentence_chars = []\n",
    "        for w in sentence:\n",
    "            spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "            sentence_chars.extend(list(spaces + w))\n",
    "        char_in = prepare_sequence(sentence_chars, char_to_idx)\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        \n",
    "        tag_scores = model(sentence_in, char_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss\n",
    "        train_examples += len(targets)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate_char(model, loss_function, optimizer):\n",
    "    # returns:: avg_val_loss (float)\n",
    "    # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            model.zero_grad()\n",
    "\n",
    "            # initiate the hidden state\n",
    "            model.hidden = model.init_hidden()\n",
    "            model.char_hidden = model.init_hidden(isChar=True)\n",
    "\n",
    "            # Get input for the model\n",
    "            sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "            sentence_chars = []\n",
    "            for w in sentence:\n",
    "                spaces = ' ' * (MAX_WORD_LEN - len(w))\n",
    "                sentence_chars.extend(list(spaces + w))\n",
    "            char_in = prepare_sequence(sentence_chars, char_to_idx)\n",
    "            targets = prepare_sequence(tags, tag_to_idx)\n",
    "\n",
    "            tag_scores = model(sentence_in, char_in)\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            \n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            \n",
    "            val_loss += loss\n",
    "            correct += torch.sum(preds == torch.LongTensor(targets))\n",
    "            val_examples += len(targets)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-QttCw6Otf-"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = CharPOSTagger(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                       char_hidden_dim = CHAR_HIDDEN_DIM, char_embedding_dim = CHAR_EMBEDDING_DIM,\n",
    "                       char_size = len(char_to_idx), vocab_size = len(word_to_idx), tagset_size = len(tag_to_idx))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    import time\n",
    "    train_char(epoch, model, loss_function, optimizer)\n",
    "    print(f\"Time used for Epoch{epoch}: \",time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xslNYW8EBKMQ"
   },
   "source": [
    "Tune your hyperparameters, to get a performance of **at least 85%** on the validation set for the CharPOSTagger.\n",
    "\n",
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuLl_BSMeovb"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du0raTJreqT2"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions_char(model, test_sentences):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis_char(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GEP-IgiESzN"
   },
   "source": [
    "\n",
    "**Report your findings here.**  \n",
    "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQdc3gH8d_a4"
   },
   "source": [
    "## Define a BiLSTM POS Tagger\n",
    "\n",
    "A bidirectional LSTM that runs both left-to-right and right-to-left to represent dependencies between adjacent words in both directions and thus captures dependencies in both directions. \n",
    "\n",
    "In this part, you make your model bidirectional. \n",
    "\n",
    "In addition, you should implement one of these modifications to improve the model's performance:\n",
    "- Tune the model hyperparameters. Try at least 5 different combinations of parameters. For example:\n",
    "    - number of LSTM layers\n",
    "    - number of hidden dimensions\n",
    "    - number of word embedding dimensions\n",
    "    - dropout rate\n",
    "    - learning rate\n",
    "- Switch to pre-trained Word Embeddings instead of training them from scratch. Try at least one different embedding method. For example:\n",
    "    - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "    - [Fast Text](https://fasttext.cc/docs/en/english-vectors.html)\n",
    "- Implement a different model architecture. Try at least one different architecture. For example:\n",
    "    - adding a conditional random field on top of the LSTM\n",
    "    - adding Viterbi decoding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext as text\n",
    "\n",
    "# Set 1: 92% acc\n",
    "# EMBEDDING_DIM = 200\n",
    "# HIDDEN_DIM = 32\n",
    "# LEARNING_RATE = 0.01\n",
    "# BIDIRECTIONAL = True\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0.1\n",
    "# EPOCHS = 50\n",
    "\n",
    "# Set 2: 91% acc\n",
    "# EMBEDDING_DIM = 200\n",
    "# HIDDEN_DIM = 16\n",
    "# LEARNING_RATE = 0.01\n",
    "# BIDIRECTIONAL = True\n",
    "# LSTM_LAYERS = 2\n",
    "# DROPOUT = 0.1\n",
    "# EPOCHS = 50\n",
    "\n",
    "# Set 3: 86% acc\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 16\n",
    "LEARNING_RATE = 0.01\n",
    "BIDIRECTIONAL = True\n",
    "LSTM_LAYERS = 4\n",
    "DROPOUT = 0.2\n",
    "EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zo-TWvcYeHhT"
   },
   "outputs": [],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    # NOTE: you may have to modify these function headers to include your \n",
    "    # modification, e.g. adding a parameter for embeddings data\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BiLSTMPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # a BiLSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.vec = text.vocab.GloVe(name='6B', dim=embedding_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size=hidden_dim, \n",
    "                        num_layers=LSTM_LAYERS, \n",
    "                        dropout = DROPOUT if LSTM_LAYERS > 1 else 0,bidirectional=BIDIRECTIONAL)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2 if BIDIRECTIONAL else hidden_dim, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        embeds = self.vec.get_vecs_by_tokens(sentence, lower_case_backup=True)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        \n",
    "        # zero the gradient\n",
    "        model.zero_grad()\n",
    "        # Prepare sentence into indexs\n",
    "#         sentence_in = prepare_sequence(sentence, word_to_idx)\n",
    "        sentence4embed = sentence\n",
    "        # Prepare tag into indexs\n",
    "        targets = prepare_sequence(tags, tag_to_idx)\n",
    "        # predictions for the tags of sentence\n",
    "        tag_scores = model(sentence4embed)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().detach().numpy()\n",
    "        train_examples += len(targets.cpu().detach().numpy())\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function, optimizer):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            # Prepare sentence into indexs\n",
    "            sentence4embed = sentence\n",
    "            # Prepare tag into indexs\n",
    "            targets = prepare_sequence(tags, tag_to_idx)\n",
    "            # predictions for the tags of sentence\n",
    "            tag_scores = model(sentence4embed)\n",
    "            # get the prediction results\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            \n",
    "            val_loss += loss.cpu().detach().numpy()\n",
    "            correct += (torch.sum(preds == torch.LongTensor(targets)).cpu().detach().numpy())\n",
    "            val_examples += len(targets.cpu().detach().numpy())\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mc2d_0k6mktK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\tAvg Train Loss: 0.0509\tAvg Val Loss: 0.0349\t Val Accuracy: 75\n",
      "Time used for Epoch1:  245.82527446746826\n",
      "Epoch: 2/50\tAvg Train Loss: 0.0300\tAvg Val Loss: 0.0289\t Val Accuracy: 80\n",
      "Time used for Epoch2:  232.62984371185303\n",
      "Epoch: 3/50\tAvg Train Loss: 0.0260\tAvg Val Loss: 0.0256\t Val Accuracy: 82\n",
      "Time used for Epoch3:  238.1525263786316\n",
      "Epoch: 4/50\tAvg Train Loss: 0.0238\tAvg Val Loss: 0.0235\t Val Accuracy: 84\n",
      "Time used for Epoch4:  225.94699621200562\n",
      "Epoch: 5/50\tAvg Train Loss: 0.0227\tAvg Val Loss: 0.0234\t Val Accuracy: 84\n",
      "Time used for Epoch5:  204.00997281074524\n",
      "Epoch: 6/50\tAvg Train Loss: 0.0218\tAvg Val Loss: 0.0231\t Val Accuracy: 84\n",
      "Time used for Epoch6:  221.0171856880188\n",
      "Epoch: 7/50\tAvg Train Loss: 0.0210\tAvg Val Loss: 0.0222\t Val Accuracy: 85\n",
      "Time used for Epoch7:  224.36360263824463\n",
      "Epoch: 8/50\tAvg Train Loss: 0.0208\tAvg Val Loss: 0.0220\t Val Accuracy: 85\n",
      "Time used for Epoch8:  226.28252744674683\n",
      "Epoch: 9/50\tAvg Train Loss: 0.0202\tAvg Val Loss: 0.0212\t Val Accuracy: 86\n",
      "Time used for Epoch9:  224.39984107017517\n",
      "Epoch: 10/50\tAvg Train Loss: 0.0198\tAvg Val Loss: 0.0212\t Val Accuracy: 86\n",
      "Time used for Epoch10:  241.88533353805542\n",
      "Epoch: 11/50\tAvg Train Loss: 0.0196\tAvg Val Loss: 0.0207\t Val Accuracy: 86\n",
      "Time used for Epoch11:  248.51661038398743\n",
      "Epoch: 12/50\tAvg Train Loss: 0.0192\tAvg Val Loss: 0.0205\t Val Accuracy: 86\n",
      "Time used for Epoch12:  237.17762732505798\n",
      "Epoch: 13/50\tAvg Train Loss: 0.0189\tAvg Val Loss: 0.0207\t Val Accuracy: 86\n",
      "Time used for Epoch13:  233.01356744766235\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-df73e95bb693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time used for Epoch{epoch}: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ec7f6c7cbdf6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, loss_function, optimizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "model = BiLSTMPOSTagger(embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                       vocab_size = len(word_to_idx), tagset_size = len(tag_to_idx))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    start = time.time()\n",
    "    train(epoch, model, loss_function, optimizer)\n",
    "    print(f\"Time used for Epoch{epoch}: \",time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6NwhSXaBCNl"
   },
   "source": [
    "Your modified model should get a performance of **at least 90%** on the validation set.\n",
    "\n",
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl2C26leFihB"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "Compare the top-10 errors made by this modified model with the errors made by the model from part (a). \n",
    "If you tried multiple hyperparameter combinations, choose the model with the highest validation data accuracy.\n",
    "What errors does the original model make as compared to the modified model, and why do you think it made them? \n",
    "\n",
    "Feel free to reuse the methods defined above for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kh0S5yXIA_0I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POS_tagging.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
